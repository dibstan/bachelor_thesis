{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-olinoDXbLXr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TORCH DEVICE: cuda\n",
            "TORCH VERSION: 1.9.1\n",
            "TORCH DEVICE: cuda\n"
          ]
        }
      ],
      "source": [
        "from utils_phys import *\n",
        "from utils_INN import * \n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm as tq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Bn1-OcNObLXw"
      },
      "outputs": [],
      "source": [
        "# Lattice Theory\n",
        "L = 14\n",
        "lattice_shape = (L,L)\n",
        "\n",
        "#Model setup\n",
        "\n",
        "n_layers = 18\n",
        "hidden_sizes = [12 ,12, 8]\n",
        "kernel_size = 3\n",
        "layers = make_phi4_affine_layers(lattice_shape=lattice_shape, n_layers=n_layers, \n",
        "    hidden_sizes=hidden_sizes, kernel_size=kernel_size)\n",
        "\n",
        "\n",
        "# Training\n",
        "base_lr = .001\n",
        "optimizer = torch.optim.Adam(layers.parameters(), lr=base_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "7eRE9v3TbLXx",
        "outputId": "0ad22a43-26af-4d44-9963-eb5bc8ec7d8b"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "base_lr = .0008\n",
        "optimizer = torch.optim.Adam(layers.parameters(), lr=base_lr)\n",
        "\n",
        "print_freq = 2\n",
        "#Training setup\n",
        "N_era = 101\n",
        "N_epoch = 200\n",
        "batch_size = 64\n",
        "history = {\n",
        "'loss' : [],\n",
        "'logp' : [],\n",
        "'logq' : [],\n",
        "'ess' : []\n",
        "}\n",
        "\n",
        "\n",
        "#Configuration setup\n",
        "lam_cur = 0.02\n",
        "kappa = [0.23, 0.25, 0.255, 0.26, 0.265, 0.27, 0.275, 0.28, 0.285, 0.29]\n",
        "\n",
        "\n",
        "#define current model with current prior\n",
        "prior = SimpleNormal(torch.zeros(lattice_shape), torch.ones(lattice_shape))\n",
        "model = {'layers': layers, 'prior': prior}\n",
        "trange = tq.trange(N_epoch*N_era)\n",
        "\n",
        "#Training scheme\n",
        "for i in trange:\n",
        "    kappa_cur_ind = np.random.randint(0, len(kappa))\n",
        "    kappa_cur = kappa[kappa_cur_ind]\n",
        "    \n",
        "    \n",
        "    \n",
        "    action_cur = ScalarPhi4Action(kappa=kappa_cur, lam=lam_cur)\n",
        "\n",
        "    #conditioning features\n",
        "    x_cond = torch.ones((batch_size, L, L)) * kappa_cur * 10\n",
        "\n",
        "    train_step(model, action_cur, calc_dkl, optimizer, history, batch_size, x_cond)\n",
        "    trange.set_postfix()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def record_flow_mc(\n",
        "        model, action_fn,\n",
        "        n_batches: int, batch_size: int, nth: int, conditioning):\n",
        "    layers, prior = model['layers'], model['prior']\n",
        "    \n",
        "    phi = []\n",
        "    accepted = []\n",
        "    mag = []\n",
        "    with torch.no_grad():\n",
        "        last_phi, last_logq = apply_flow_to_prior(prior, layers, conditioning, batch_size = batch_size)\n",
        "        last_logp = -action_fn(last_phi)\n",
        " \n",
        "        trange = tq.trange(n_batches)\n",
        "        for i in trange:\n",
        "            new_phi, new_logq = apply_flow_to_prior(prior, layers, conditioning, batch_size = batch_size)\n",
        "            new_logp = -action_fn(new_phi)\n",
        "            \n",
        "            p_accept = torch.exp((new_logp - new_logq) - (last_logp - last_logq))\n",
        "            accept = torch.rand(p_accept.shape) < p_accept\n",
        "            accepted.append(grab(accept))\n",
        "            a = accept.view(-1,1,1).repeat(1,*new_phi.shape[1:])\n",
        "\n",
        "            last_phi = a * new_phi + ~a * last_phi\n",
        "            last_logq = accept * new_logq + ~accept * last_logq\n",
        "            last_logp = accept * new_logp + ~accept * last_logp\n",
        "\n",
        "            M = get_mag(last_phi)\n",
        "            ind = np.where(M < 0)\n",
        "            M[ind] = M[ind] * -1 \n",
        "            mag.append(M)\n",
        "            if (i+1) % nth == 0:\n",
        "                phi.append(grab(last_phi))\n",
        " \n",
        "            trange.set_postfix(acc=str(np.round(np.mean(np.array(accepted)), 3)))\n",
        " \n",
        "    print(f\"acceptance rate: {np.mean(accepted)}\")\n",
        " \n",
        "    \n",
        "    phi = np.concatenate(phi)\n",
        "    mag = jackknife(np.concatenate(mag))\n",
        "    \n",
        "    return {\"phi\" : phi,\n",
        "            \"accepted\": accepted,\n",
        "            \"M\" : mag}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ur6luC-jbLX3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def sample_MC(start, stop, step, model, batch_size, nr_batches, lam_cur, lattice_dim):\n",
        "    kappas = np.arange(start, stop, step)\n",
        "    chis = []\n",
        "    mag = []\n",
        "    acc_rate = []\n",
        "    for kappa in kappas:\n",
        "\n",
        "        action_cur = ScalarPhi4Action(kappa=kappa, lam=lam_cur)\n",
        "        x_cond = torch.ones((batch_size, lattice_dim, lattice_dim)) * kappa * 10\n",
        "        \n",
        "        phi4_ens = record_flow_mc(model, action_cur, nr_batches, batch_size, 6, x_cond)\n",
        "        \n",
        "        x = phi4_ens[\"phi\"]\n",
        "        ind = np.where(np.sum(x, axis = (1,2)) < 0)\n",
        "        x[ind] = x[ind] * -1\n",
        "        \n",
        "        chi = get_chi2(x)\n",
        "        chis.append(chi)\n",
        "        mag.append(phi4_ens[\"M\"])\n",
        "        acc_rate.append(np.mean(np.array(phi4_ens['accepted'])))\n",
        "    \n",
        "    return chis, mag, kappas, acc_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF7Ku0XDbLX4",
        "outputId": "061bdbe9-1c25-4392-ea10-a012a3759b00"
      },
      "outputs": [],
      "source": [
        "chi_14, mag_14, kappa_plot, acc_rate = sample_MC(0.25, 0.28, 0.002, model, 64, 100, lam_cur, 14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lattice_inn_4.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "6f31f2af74753254a8f1dfb220c0180b4c04469a91f6ccc4ace7392572795ab4"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('bachelor_env': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
